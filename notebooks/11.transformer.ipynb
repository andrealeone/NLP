{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ba027a",
   "metadata": {},
   "source": [
    "<br>\n",
    "<u>Notebook Eleven</u> | \n",
    "<a href=https://leone.gdn/NLP target=_blank>Report</a> | \n",
    "<a href=https://github.com/andrealeone/NLP target=_blank>Repository</a>\n",
    "<br><br>\n",
    "<b>Attention Is All You Need — Transformer [experiment]</b><br><br>\n",
    "Andrea Leone<br>\n",
    "ML for NLP — University of Trento<br>\n",
    "January 2022\n",
    "<hr><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer\n",
    "from tokenizers.models import WordLevel, BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace,WhitespaceSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0d54e",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665abbe4",
   "metadata": {},
   "source": [
    "*Hic sunt Leones*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc84597",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065690e",
   "metadata": {},
   "source": [
    "**Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "\n",
    "    # RUN CONFIG:\n",
    "    RUN_NAME                = 'unofficial_single_gpu_run',\n",
    "    RUN_DESCRIPTION         = 'No description',\n",
    "    RUNS_FOLDER_PTH         = '../runs',\n",
    "\n",
    "    # DATA CONFIG:\n",
    "    DATASET_SIZE            =  30000,\n",
    "    TEST_PROPORTION         =  0.01,\n",
    "    MAX_SEQ_LEN             =  40,\n",
    "    VOCAB_SIZE              =  60000,\n",
    "    TOKENIZER_TYPE          = 'wordlevel',\n",
    "\n",
    "    # TRAINING CONFIG:\n",
    "    BATCH_SIZE              =  48, \n",
    "    GRAD_ACCUMULATION_STEPS =  2048//48,\n",
    "    WORKER_COUNT            =  10,\n",
    "    EPOCHS                  =  100,\n",
    "\n",
    "    # OPTIMIZER CONFIG:\n",
    "    BETAS                   = (0.9, 0.98),\n",
    "    EPS                     =  1e-9,\n",
    "\n",
    "    # SCHEDULER CONFIG:\n",
    "    N_WARMUP_STEPS          =  4000, \n",
    "\n",
    "    # MODEL CONFIG:\n",
    "    D_MODEL                 =  512,\n",
    "    N_BLOCKS                =  6,\n",
    "    N_HEADS                 =  8,\n",
    "    D_FF                    =  2048,\n",
    "    DROPOUT_PROBA           =  0.1,\n",
    "\n",
    "    # OTHER:\n",
    "    MODEL_SAVE_EPOCH_CNT    =  10,\n",
    "    DEVICE                  = 'gpu',\n",
    "    LABEL_SMOOTHING         =  0.1\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70554145",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135d49f",
   "metadata": {},
   "source": [
    "**Dataset & Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0ba75",
   "metadata": {},
   "source": [
    "Authors of the paper used the WMT 2014 English-German dataset consisting of 4.5 million sentence pairs, same is used here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834db66",
   "metadata": {},
   "source": [
    " For loading the dataset we will use the HuggingFace Datasets library which will help us download and generally manipulate the dataset much easier. DATASET_SIZE parameter specified in config let's us select only a part of the dataset if we do not wish to train on the whole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e48c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_dataset('wmt14','de-en',split='train').shuffle(seed=42)\n",
    "data=data.select(range(config['DATASET_SIZE'])) \n",
    "data=data.flatten()\n",
    "data=data.rename_column('translation.de','translation_trg')\n",
    "data=data.rename_column('translation.en','translation_src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55da2804",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c24fa",
   "metadata": {},
   "source": [
    "**Tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e7ffff",
   "metadata": {},
   "source": [
    "For creating the tokenizer we use the HuggingFace Tokenizers library. In the paper they used a single BPE tokenizer trained on sentences from both languages. A word level tokenizer is selected here instead, as I found that for my simpler training configurations it worked better. Also note that if choosing a word-level tokenizer the vocabulary size (VOCAB_SIZE param) needs to be increased compared to the 37000 word vocabulary for the BPE mentioned in the paper.\n",
    "\n",
    "The process of creating a tokenizer boils down to selecting a tokenization model and customizing its components. For more info on the components I used here, see the [hugging face docs](https://huggingface.co/docs/tokenizers/python/latest/components.html)\n",
    "\n",
    "[BOS], [EOS], [PAD] and [UNK] tokens are also added. [BOS] token is useful in the decoder input to signalize the beggining of a sentece, remember that the original transformer decoder predicts the next word in the sequence by looking at the encoder representation and the decoder input up to the current timestep.Therefore for predicting the first word it only sees the [BOS]. [EOS] token signalizes the end of the sequence and therefore the end of decoding when inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer=normalizers.Sequence([NFD(),StripAccents(), Lowercase()])\n",
    "tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "trainer_src = WordLevelTrainer(vocab_size=config['VOCAB_SIZE'], special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]) \n",
    "\n",
    "# Configure batch iterators to train tokenizers from memory\n",
    "def batch_iterator_src(batch_size=10000):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i : i + batch_size]['translation_src']\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i : i + batch_size]['translation_trg']\n",
    "\n",
    "# Train tokenizers\n",
    "tokenizer.train_from_iterator(batch_iterator_src(), trainer=trainer_src, length=len(data))\n",
    "\n",
    "# Configure postprocessing to add [BOS] and [EOS] tokens to trg sequence\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $A [EOS]\",\n",
    "    special_tokens=[\n",
    "        (\"[BOS]\", 2),\n",
    "        (\"[EOS]\", 3),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b102b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding=tokenizer.encode('In diesem Rubrik finden Sie Fahndungsmeldungen, die auf Anfrage eines Staatsanwalts oder Untersuchungsrichter verbreitet werden.')\n",
    "\n",
    "print('tokens: ', encoding.tokens)\n",
    "print('ids: ',encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f5706c",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c77d0",
   "metadata": {},
   "source": [
    "**Preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc8c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, tokenizer, max_seq_len, test_proportion):\n",
    "\n",
    "    # Tokenize\n",
    "    def tokenize(example):\n",
    "        return {\n",
    "            'translation_src': tokenizer.encode(example['translation_src']).ids,\n",
    "            'translation_trg': tokenizer.encode(example['translation_trg']).ids,\n",
    "        }\n",
    "    data=data.map(tokenize)\n",
    "\n",
    "    # Compute sequence lengths\n",
    "    def sequence_length(example):\n",
    "        return {\n",
    "            'length_src': [len(item) for item in example['translation_src']],\n",
    "            'length_trg': [len(item) for item in example['translation_trg']],\n",
    "        }\n",
    "    data=data.map(sequence_length, batched=True, batch_size=10000)\n",
    "\n",
    "    # Filter by sequence lengths\n",
    "    def filter_long(example):\n",
    "        return example['length_src']<= max_seq_len and example['length_trg']<=max_seq_len\n",
    "    data=data.filter(filter_long)\n",
    "\n",
    "    # Split \n",
    "    data=data.train_test_split(test_size=test_proportion)\n",
    "\n",
    "    # Sort each split by length for dynamic batching (see CustomBatchSampler)\n",
    "    data['train']=data['train'].sort('length_src', reverse=True)\n",
    "    data['test']=data['test'].sort('length_src', reverse=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d933b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=preprocess_data(data, tokenizer, config['MAX_SEQ_LEN'], config['TEST_PROPORTION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b3357d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3edc590",
   "metadata": {},
   "source": [
    "**Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1636f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_encoded=self.dataset[idx]['translation_src']\n",
    "        trg_encoded=self.dataset[idx]['translation_trg']\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(src_encoded),\n",
    "            torch.tensor(trg_encoded),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ada6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=TranslationDataset(data['train'])\n",
    "val_ds=TranslationDataset(data['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456d00c",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc957b4",
   "metadata": {},
   "source": [
    "**Collate function** for padding sequences in a batch to same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9055f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_fn(batch):\n",
    "    src_sentences,trg_sentences=[],[]\n",
    "    for sample in batch:\n",
    "        src_sentences+=[sample[0]]\n",
    "        trg_sentences+=[sample[1]]\n",
    "\n",
    "    src_sentences = pad_sequence(src_sentences, batch_first=True, padding_value=0)\n",
    "    trg_sentences = pad_sequence(trg_sentences, batch_first=True, padding_value=0)\n",
    "\n",
    "    return src_sentences, trg_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c923d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example-use\n",
    "batch=[\n",
    "    (torch.tensor([1,2,3,4]),torch.tensor([1,2,3,4,5,6,7])), #x's\n",
    "    (torch.tensor([1,2,3,4,5,6]), torch.tensor([1,2]))  # y's\n",
    "]\n",
    "\n",
    "pad_collate_fn(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63b052",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ceafa",
   "metadata": {},
   "source": [
    "**Batch Sampler** for sampling sequences of similar lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddd62b",
   "metadata": {},
   "source": [
    "Batch sampler ensures that batches contain sequences of similar lengths as explained before. It iteratively returns indices of samples that should go together in a batch. We already sorted the splits by length so here we just chunk indices of sorted elements in order. We also care to shuffle the batches here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(indices, chunk_size):\n",
    "    return torch.split(torch.tensor(indices), chunk_size)\n",
    "\n",
    "class CustomBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "\n",
    "        # Dataset is already sorted so just chunk indices\n",
    "        # into batches of indices for sampling\n",
    "        self.batch_size=batch_size\n",
    "        self.indices=range(len(dataset))\n",
    "        self.batch_of_indices=list(chunk(self.indices, self.batch_size))\n",
    "        self.batch_of_indices = [batch.tolist() for batch in self.batch_of_indices]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.batch_of_indices)\n",
    "        return iter(self.batch_of_indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.batch_of_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5144a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_batcher_train = CustomBatchSampler(train_ds, config['BATCH_SIZE'])\n",
    "custom_batcher_val= CustomBatchSampler(val_ds, config['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aaabe5",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6486e34",
   "metadata": {},
   "source": [
    "**DataLoaders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83357959",
   "metadata": {},
   "source": [
    "Next, Dataloaders are constructed with the described collate and batch sampling policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example-use\n",
    "dummy_batcher = CustomBatchSampler(train_ds, 3)\n",
    "dummy_dl=DataLoader(train_ds, collate_fn=pad_collate_fn, batch_sampler=dummy_batcher, pin_memory=True)\n",
    "for batch in dummy_dl:\n",
    "    print('Shapes: ')\n",
    "    print('-'*10)\n",
    "    print(batch[0].size())\n",
    "    print(batch[1].size())\n",
    "    print()\n",
    "    \n",
    "\n",
    "    print('e.g. src batch (see there is minimal/no padding):')\n",
    "    print('-'*10)\n",
    "    print(batch[0].numpy())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl=DataLoader(train_ds, collate_fn=pad_collate_fn, batch_sampler=custom_batcher_train, pin_memory=True)\n",
    "val_dl=DataLoader(val_ds, collate_fn=pad_collate_fn, batch_sampler=custom_batcher_val, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df096f",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc8627",
   "metadata": {},
   "source": [
    "### Transformer Architecture\n",
    "For explaining the architecture I chose the bottom-up approach. First I will describe the basic building blocks and then gradually build up the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e1cf9",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430b69e",
   "metadata": {},
   "source": [
    "**Positional Embedding Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac10f19",
   "metadata": {},
   "source": [
    "*Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence,we must inject some information about the relative or absolute position of the tokens in the sequence.*\n",
    "\n",
    "The attention mechanism in the transformer, compared to RNN's, doesn't \"contain\" the concept of time in its architecture (e.g. attention doesn't care about the position of tokens in a sequence, it inheritely views it all the same, that is why it can be parallelized), therefore we have to somehow **embed the position (time-step) information into the word embeddings** inputed to the attention mechanism.\n",
    "\n",
    "Authors solve this by **adding** (yes, just adding, not concatenating) **precomputed positional encodings to word embeddings**. Positional encodings are defined as sine and cosine functions of different frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee42b42",
   "metadata": {},
   "source": [
    "*Where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
    "corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
    "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
    "relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\n",
    "PEpos.*\n",
    "\n",
    "\n",
    "Also dropout is applied to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): \n",
    "\n",
    "    def __init__(self, d_model, max_seq_len=500, dropout_proba=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.d_model=d_model\n",
    "\n",
    "        pe_table=self.get_pe_table()\n",
    "        self.register_buffer('pe_table' , pe_table)\n",
    "\n",
    "        self.dropout=nn.Dropout(dropout_proba) \n",
    "\n",
    "    def get_pe_table(self):\n",
    "\n",
    "        position_idxs  = torch.arange( self.max_seq_len ).unsqueeze(1) \n",
    "        embedding_idxs = torch.arange( self.d_model     ).unsqueeze(0)\n",
    "        \n",
    "        angle_rads = position_idxs * 1 / torch.pow(10000, ( 2 * ( embedding_idxs // 2 ) ) / self.d_model )\n",
    "\n",
    "        angle_rads[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pe_table = angle_rads.unsqueeze(0)\n",
    "\n",
    "        return pe_table\n",
    "\n",
    "    def forward(self, embeddings_batch):\n",
    "\n",
    "        seq_len = embeddings_batch.size(1)\n",
    "        pe_batch = self.pe_table[:, :seq_len].clone().detach()\n",
    "\n",
    "        return self.dropout(embeddings_batch + pe_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e9933",
   "metadata": {},
   "source": [
    "To understand what is actually being added take a look at the below visualization of positional encodings.\n",
    "\n",
    "**Each row *i* of the *pe_table* represents the vector that would be added to a word at position *i***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084d5af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "positional_encoding=PositionalEncoding(512, 500)\n",
    "pe_table=positional_encoding.get_pe_table()\n",
    "\n",
    "plt.imshow(pe_table.squeeze(0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188d934",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb946bc",
   "metadata": {},
   "source": [
    "**Add & Norm Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb89937",
   "metadata": {},
   "source": [
    "Helps propagate gradients easier and speeds up learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b84366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddAndNorm(nn.Module): \n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super(AddAndNorm, self).__init__()\n",
    "        \n",
    "        self.layer_norm=nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "\n",
    "        return self.layer_norm(x+residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd36cf51",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ff0ba",
   "metadata": {},
   "source": [
    "**Position-wise Feed Forward layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c54e696",
   "metadata": {},
   "source": [
    "*In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
    "connected feed-forward network, which is applied to each position separately and identically. This\n",
    "consists of two linear transformations with a ReLU activation in between.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d2ef7",
   "metadata": {},
   "source": [
    "*While the linear transformations are the same across different positions, they use different parameters\n",
    "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
    "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
    "dff = 2048.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForwardNet(nn.Module): \n",
    "    \n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForwardNet, self).__init__()\n",
    "        \n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(torch.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334482d8",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b96721",
   "metadata": {},
   "source": [
    "**Multi Head Attention Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb25f684",
   "metadata": {},
   "source": [
    "Core of the transformer is the attention mechanism which enables creating **modified word representations** (attention representations) **that take into account the word's meaning in relation to other words in a sequence** (e.g. the word \"bank\" can represent a financial institution or a land along the edge of a river as in \"river bank\"). Depending on how we think about a word we may choose to represent it differently. This transcends the limits of traditional word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea8dfa1",
   "metadata": {},
   "source": [
    "**Scaled Dot Product Attention Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c304496",
   "metadata": {},
   "source": [
    "This is not meant to be an attention tutorial but I will just briefly give an intuitive reasoning on how does attention accomplish its task of creating context based embeddings of words.\n",
    "\n",
    "ATTENTION MECHANISM STEPS:\n",
    "- Obtain a **regular word embedding** for each word in a sequence.\n",
    "- With **learnable linear layers** create query(q), key(k) and value(v) representations for each word. You can think of these - intuitively as follows:\n",
    "    - Query represents a representations of a question you get to ask about that word.\n",
    "    - Key represents a representation of an answer w.r.t the question.\n",
    "    - Value represents a word representations that we will weigh with attention weights computed in the following steps.\n",
    "- **For each word's query calculate the match with each key using a dot product** (this dot product produces the attention weight, therefore dot product that produces a greater value signals that there is an important connection between those words that is important for understanding (and to which we should pay attention to), e.g the river bank analogy I used before).\n",
    "- Once we have all the attention weights between pairs of queries and keys we **apply softmax** to restrict the attention weight values to [0,1] range.\n",
    "- Finally, to get an attention representation for a word we **multiply attention weights** (which came from this word's query and all the keys) **with each word's value representation and sum all of them**.\n",
    "\n",
    "This will yield a word representation that is aware of its context.\n",
    "\n",
    "PARALLELIZATION\n",
    "\n",
    "When parallelized and batched all of this can be condensed to the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910b40c",
   "metadata": {},
   "source": [
    "The division by the square root of the model dimension is justified by:\n",
    "*We suspect that for large model dimensions, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by 1/sqrt(model_dimension)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15026475",
   "metadata": {},
   "source": [
    "MASKING\n",
    "\n",
    "One thing missing from the formula is the mask that is applied before softmax inside the attention mechanism. When applied, the mask sets all values that correspond to **unwanted connections to minus infinity**. \n",
    "\n",
    "There are two types used: \n",
    "- Padding mask\n",
    "- Lookahead mask. \n",
    "\n",
    "Padding mask prevents the attention mechanism inside the encoder to pay attention to padding tokens. Lookahead mask, used in the decoder, additionally prevents attending to positions over the current position.\n",
    "\n",
    "When implemented in code it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30270be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module): \n",
    "\n",
    "    def __init__(self, d_head):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "        self.d_head = d_head\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        attention_weights = torch.matmul(q, k.transpose(-2, -1))\n",
    "        scaled_attention_weights = attention_weights / math.sqrt(self.d_head)\n",
    "\n",
    "        if mask is not None:\n",
    "\n",
    "            # (batch_size, n_heads, seq_len, seq_len)\n",
    "            scaled_attention_weights = scaled_attention_weights.masked_fill(\n",
    "                mask == 0, float('-inf')\n",
    "            )\n",
    "\n",
    "        # Apply softmax over the last dimension which corresponds to attention weights for a key \n",
    "        # (batch_size, n_heads, seq_len, seq_len)\n",
    "        scaled_attention_weights = nn.functional.softmax(scaled_attention_weights, dim=-1)\n",
    "\n",
    "        # Optional dropout (not mentioned in the paper)\n",
    "        scaled_attention_weights = self.attention_dropout(scaled_attention_weights) # (batch_size, n_heads, seq_len, seq_len)\n",
    "\n",
    "        weighted_v = torch.matmul(scaled_attention_weights, v) # (batch_size, n_heads, seq_len, d_head)\n",
    "\n",
    "        return weighted_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43d11e5",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680cfb2c",
   "metadata": {},
   "source": [
    "**Multi Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807af4b",
   "metadata": {},
   "source": [
    "It was found beneficial to **project input to different *Q*s, *K*s and *V*s *n_head* times**. This way each head projects to a smaller dimension equal to *d_model / d_head* in order to keep the computational complexity the same. Intuitively this enables the network to ask more questions with different queries. In other words it gives multiple representation subspaces. It can also be thought of as a for loop over the attention mechanism. Also notice an additional linear layer *W0*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61de86e",
   "metadata": {},
   "source": [
    "Multi head attention can also be parallelized since no one head's value depends on the value of any other head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): \n",
    "\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.n_heads= n_heads\n",
    "\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        self.dot_product_attention_layer= ScaledDotProductAttention(self.d_head)\n",
    "\n",
    "        self.W_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def _split_into_heads(self, q,k,v):\n",
    "\n",
    "        q= q.view(q.size(0), q.size(1), self.n_heads, self.d_head)\n",
    "        k= k.view(k.size(0), k.size(1), self.n_heads, self.d_head)\n",
    "        v= v.view(v.size(0), v.size(1), self.n_heads, self.d_head)\n",
    "\n",
    "        q= q.transpose(1,2)\n",
    "        k= k.transpose(1,2)\n",
    "        v= v.transpose(1,2)\n",
    "\n",
    "        return q,k,v\n",
    "\n",
    "    def _concatenate_heads(self,attention_output):\n",
    "\n",
    "        attention_output = attention_output.transpose(1,2).contiguous()\n",
    "        attention_output = attention_output.view(attention_output.size(0), attention_output.size(1), -1)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        q, k, v = self._split_into_heads(q,k,v)\n",
    "\n",
    "        attention_output = self.dot_product_attention_layer(q, k, v, mask)\n",
    "        attention_output = self._concatenate_heads(attention_output)\n",
    "\n",
    "        attention_output = self.W_0(attention_output)\n",
    "\n",
    "        return attention_output \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f4cc7",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7884108",
   "metadata": {},
   "source": [
    "**Transformer Encoder Block**\n",
    "  \n",
    "Encoder's job is to **process the source sequence and output it's word embeddings fused with attention representation & positional encoding for use in the decoder**. \n",
    "\n",
    "The encoder blocks are **stacked *N* times** each feeding its output to the next one's input (word embedding and positional encoding layers are only applied before the first encoder block)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19189b3d",
   "metadata": {},
   "source": [
    "**Note:** Only the output of the last encoder block will ever be considered by the decoder.\n",
    "\n",
    "Additional implementation details include:\n",
    "- Dropout is applied to the output of each sub-layer, before it is added to the\n",
    "sub-layer input and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4400857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module): \n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout_proba):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model) \n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.mha_layer = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "        self.dropout_layer_1      = nn.Dropout(dropout_proba)\n",
    "        self.add_and_norm_layer_1 = AddAndNorm(d_model)\n",
    "\n",
    "        self.ffn_layer = PositionWiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.dropout_layer_2      = nn.Dropout(dropout_proba)\n",
    "        self.add_and_norm_layer_2 = AddAndNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        mha_out = self.mha_layer(q, k, v, mask)\n",
    "        mha_out = self.dropout_layer_1(mha_out)\n",
    "        mha_out = self.add_and_norm_layer_1(x, mha_out)\n",
    "\n",
    "        ffn_out = self.ffn_layer(mha_out)\n",
    "        ffn_out = self.dropout_layer_2(ffn_out)\n",
    "        ffn_out = self.add_and_norm_layer_2(mha_out, ffn_out)\n",
    "\n",
    "        return ffn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444a727",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412315a5",
   "metadata": {},
   "source": [
    "**Transformer Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module): \n",
    "\n",
    "    def __init__(self, n_blocks, n_heads, d_model, d_ff, dropout_proba=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.encoder_blocks=nn.ModuleList([TransformerEncoderBlock(d_model, n_heads, d_ff, dropout_proba) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x, mask): \n",
    "\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a9dc6",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f50023",
   "metadata": {},
   "source": [
    "**Transformer Decoder Block**\n",
    "  \n",
    "Decoder's job is to **process the target sequence with consideraton to the encoder output and output it's word embeddings fused with attention representation & positional encoding for predicting the next token**.\n",
    "\n",
    "The decoder block is also repeated N times but unlike the encoder it has an **additional attention layer**. This layer is the **Encoder-Decoder attention** layer which **pulls context from the last encoder block's output** at each decoder step, helping it to decode.\n",
    "\n",
    "During training the decoder predictions can be parallelized because we have the target tokens which we use in a teacher-forcing manner, but inference is done in an autoregressive manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf897aee",
   "metadata": {},
   "source": [
    "Additional implementation details include:\n",
    "- Dropout is applied to the output of each sub-layer, before it is added to the\n",
    "sub-layer input and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module): \n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout_proba):\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "\n",
    "        self.W_q_1 = nn.Linear(d_model, d_model)\n",
    "        self.W_k_1 = nn.Linear(d_model, d_model)\n",
    "        self.W_v_1 = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.mha_layer_1     = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "        self.dropout_layer_1 = nn.Dropout(dropout_proba)\n",
    "        self.add_and_norm_1  = AddAndNorm(d_model)\n",
    "\n",
    "        self.W_q_2 = nn.Linear(d_model, d_model)\n",
    "        self.W_k_2 = nn.Linear(d_model, d_model)\n",
    "        self.W_v_2 = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.mha_layer_2     = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "        self.dropout_layer_2 = nn.Dropout(dropout_proba)\n",
    "        self.add_and_norm_2  = AddAndNorm(d_model)\n",
    "\n",
    "        self.ffn_layer = PositionWiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.dropout_layer_3 = nn.Dropout(dropout_proba)\n",
    "        self.add_and_norm_3  = AddAndNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, trg_mask):\n",
    "\n",
    "        q_1 = self.W_q_1(x)\n",
    "        k_1 = self.W_k_1(x)\n",
    "        v_1 = self.W_v_1(x)\n",
    "\n",
    "        mha_layer_1_out = self.mha_layer_1(q_1, k_1, v_1, trg_mask)\n",
    "        mha_layer_1_out = self.dropout_layer_1(mha_layer_1_out)\n",
    "        mha_layer_1_out = self.add_and_norm_1(mha_layer_1_out, x)\n",
    "\n",
    "        q_2 = self.W_q_2(mha_layer_1_out)\n",
    "        k_2 = self.W_k_2(encoder_output)\n",
    "        v_2 = self.W_v_2(encoder_output)\n",
    "\n",
    "        mha_layer_2_out = self.mha_layer_2(q_2, k_2, v_2, src_mask)\n",
    "        mha_layer_2_out = self.dropout_layer_2(mha_layer_2_out)\n",
    "        mha_layer_2_out = self.add_and_norm_2(mha_layer_2_out, mha_layer_1_out)\n",
    "\n",
    "        ffn_out = self.ffn_layer(mha_layer_2_out)\n",
    "        ffn_out = self.dropout_layer_3(ffn_out)\n",
    "        ffn_out = self.add_and_norm_3(ffn_out, mha_layer_2_out)\n",
    "        \n",
    "        return ffn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612ad18",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6430c9",
   "metadata": {},
   "source": [
    "**Transformer Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdbe5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module): \n",
    "\n",
    "    def __init__(self, n_blocks, n_heads, d_model, d_ff, dropout_proba):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.decoder_blocks=nn.ModuleList([ \n",
    "\n",
    "            TransformerDecoderBlock( d_model, n_heads, d_ff, dropout_proba )\n",
    "            for _ in range(n_blocks)\n",
    "\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, trg_mask): \n",
    "\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(x, encoder_output, src_mask, trg_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d40f6d",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d1fb3",
   "metadata": {},
   "source": [
    "**Full Encoder-Decoder Transformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd70ff",
   "metadata": {},
   "source": [
    "Encoder and decoder are connected in such a way that each decoder block can pull context from the decoder output.\n",
    "\n",
    "Again, only the output of the last encoder block will ever be considered by the decoder which can be misleading looking at the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ce422",
   "metadata": {},
   "source": [
    "Additional implementation details include:\n",
    "- Weights are shared between two embedding layers and the pre-softmax linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module): \n",
    "\n",
    "    def __init__(self,d_model, n_blocks, src_vocab_size, trg_vocab_size, n_heads, d_ff, dropout_proba):\n",
    "        super(TransformerEncoderDecoder, self).__init__()\n",
    "\n",
    "        self.dropout_proba = dropout_proba\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Encoder part\n",
    "        self.src_embedding     = nn.Embedding( src_vocab_size, d_model )\n",
    "        self.src_pos_embedding = PositionalEncoding( d_model )\n",
    "        self.encoder           = TransformerEncoder( n_blocks, n_heads, d_model, d_ff, dropout_proba )\n",
    "\n",
    "        # Decoder part \n",
    "        self.trg_embedding     = nn.Embedding( trg_vocab_size, d_model )\n",
    "        self.trg_pos_embedding = PositionalEncoding( d_model )\n",
    "        self.decoder           = TransformerDecoder( n_blocks, n_heads, d_model, d_ff, dropout_proba )\n",
    "\n",
    "        # Linear mapping to vocab size\n",
    "        self.linear = nn.Linear( d_model, trg_vocab_size )\n",
    "\n",
    "        # Switch to xavier initialization (shown to be beneficial)\n",
    "        self.init_with_xavier()\n",
    "\n",
    "        # Sharing weights between two embedding layers and the pre-softmax linear layer\n",
    "        self.src_embedding.weight = self.trg_embedding.weight\n",
    "        self.linear.weight        = self.trg_embedding.weight\n",
    "\n",
    "    def encode(self, src_token_ids, src_mask): \n",
    "\n",
    "        # Encoder part\n",
    "        src_embeddings  = self.src_embedding(src_token_ids) * math.sqrt(self.d_model)\n",
    "        src_embeddings  = self.src_pos_embedding(src_embeddings)\n",
    "        encoder_outputs = self.encoder(src_embeddings, src_mask)\n",
    "\n",
    "        return encoder_outputs\n",
    "\n",
    "    def decode(self, trg_token_ids, encoder_outputs, src_mask, trg_mask): \n",
    "\n",
    "        # Decoder part\n",
    "        trg_embeddings  = self.trg_embedding(trg_token_ids) * math.sqrt(self.d_model)\n",
    "        trg_embeddings  = self.trg_pos_embedding(trg_embeddings)\n",
    "        decoder_outputs = self.decoder(trg_embeddings, encoder_outputs, src_mask, trg_mask)\n",
    "\n",
    "        # Linear mapping to vocab size\n",
    "        linear_out = self.linear(decoder_outputs)\n",
    "\n",
    "        return linear_out\n",
    "\n",
    "    def forward(self, src_token_ids, trg_token_ids, src_mask, trg_mask):\n",
    "\n",
    "        encoder_outputs = self.encode(src_token_ids, src_mask)\n",
    "        decoder_outputs = self.decode(trg_token_ids, encoder_outputs, src_mask, trg_mask)\n",
    "\n",
    "        return decoder_outputs \n",
    "\n",
    "    def init_with_xavier(self):\n",
    "\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d72d7",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a36721",
   "metadata": {},
   "source": [
    "Once we have the transformer architecture we need to take care of some \"preprocessing\" and \"postprocessing\" details related to it's use. That is why we will wrap it into MachineTranslationTransformer class which will additionally handle the following:\n",
    "- Creating pad mask.\n",
    "- Creating lookahead mask.\n",
    "- Sentence tokenization with the aid of a tokenizer and input preparation.\n",
    "- Autoregressive decoding for inference (greedy decoding used). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0027b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineTranslationTransformer(nn.Module): \n",
    "\n",
    "    def __init__(self, d_model,n_blocks,src_vocab_size,trg_vocab_size,n_heads,d_ff, dropout_proba):\n",
    "        super(MachineTranslationTransformer, self).__init__()\n",
    "\n",
    "        self.transformer_encoder_decoder=TransformerEncoderDecoder(\n",
    "            d_model, n_blocks,\n",
    "            src_vocab_size, trg_vocab_size,\n",
    "            n_heads, d_ff,\n",
    "            dropout_proba\n",
    "        )\n",
    "\n",
    "    def _get_pad_mask(self, token_ids, pad_idx=0): \n",
    "        \n",
    "        pad_mask= (token_ids != pad_idx).unsqueeze(-2) # (batch_size, 1, seq_len)\n",
    "        return pad_mask.unsqueeze(1)\n",
    "\n",
    "    def _get_lookahead_mask(self, token_ids): \n",
    "        \n",
    "        sz_b, len_s = token_ids.size()\n",
    "        subsequent_mask = (\n",
    "            1 - torch.triu(\n",
    "                torch.ones( (1, len_s, len_s), device=token_ids.device ), diagonal=1\n",
    "            )\n",
    "        ).bool()\n",
    "        \n",
    "        return subsequent_mask.unsqueeze(1)\n",
    "\n",
    "    def forward(self, src_token_ids, trg_token_ids):\n",
    "\n",
    "        # Since trg_token_ids contains both [BOS] and [SOS] tokens\n",
    "        # we need to remove the [EOS] token when using it as input to the decoder.\n",
    "        # Similarly we remove the [BOS] token when we use it as y to calculate loss,\n",
    "        # which also makes y and y_pred shapes match.\n",
    "\n",
    "        trg_token_ids=trg_token_ids[:, :-1]\n",
    "\n",
    "        src_mask = self._get_pad_mask(src_token_ids)\n",
    "        trg_mask = self._get_pad_mask(trg_token_ids) & self._get_lookahead_mask(trg_token_ids)\n",
    "\n",
    "        return self.transformer_encoder_decoder(\n",
    "            src_token_ids, trg_token_ids, src_mask, trg_mask\n",
    "        )\n",
    "\n",
    "    def preprocess(self, sentence, tokenizer):\n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        src_token_ids = tokenizer.encode(sentence).ids\n",
    "        src_token_ids = torch.tensor(src_token_ids, dtype=torch.long).to(device)\n",
    "        src_token_ids = src_token_ids.unsqueeze(0)\n",
    "\n",
    "        return src_token_ids\n",
    "\n",
    "    def translate(self, sentence, tokenizer, max_tokens=100, skip_special_tokens=False):\n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        eos_id = tokenizer.token_to_id('[EOS]')\n",
    "        bos_id = tokenizer.token_to_id('[BOS]')\n",
    "        pad_id = tokenizer.token_to_id('[PAD]')\n",
    "\n",
    "        src_token_ids = self.preprocess(sentence, tokenizer)\n",
    "\n",
    "        trg_token_ids = torch.LongTensor([bos_id]).unsqueeze(0).to(device)\n",
    "\n",
    "        src_mask = self._get_pad_mask( src_token_ids )\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        encoder_output = self.transformer_encoder_decoder.encode( src_token_ids, src_mask )\n",
    "\n",
    "        while True:\n",
    "\n",
    "            trg_mask = self._get_lookahead_mask(trg_token_ids)\n",
    "            decoder_output = self.transformer_encoder_decoder.decode(\n",
    "                trg_token_ids, encoder_output, src_mask, trg_mask\n",
    "            )\n",
    "\n",
    "            softmax_output = nn.functional.log_softmax( decoder_output, dim=-1 )\n",
    "            softmax_output_last = softmax_output[:, -1, :]\n",
    "\n",
    "            _, token_id = softmax_output_last.max(dim=-1)\n",
    "\n",
    "            if token_id.item() == eos_id or trg_token_ids.size(1) == max_tokens: \n",
    "                trg_token_ids = torch.cat([trg_token_ids, token_id.unsqueeze(0)], dim=-1)\n",
    "                break\n",
    "\n",
    "            trg_token_ids = torch.cat(\n",
    "                [trg_token_ids, token_id.unsqueeze(0)], dim=-1\n",
    "            )\n",
    "\n",
    "        decoded_output = tokenizer.decode(\n",
    "            trg_token_ids.squeeze(0).detach().cpu().numpy(),\n",
    "            skip_special_tokens = skip_special_tokens\n",
    "        )\n",
    "\n",
    "        return decoded_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af48335",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93021428",
   "metadata": {},
   "source": [
    "**Training Loop** //"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2464b4",
   "metadata": {},
   "source": [
    "**Custom Scheduler**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc3fab",
   "metadata": {},
   "source": [
    "Authors used a custom scheduler when training: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645fb357",
   "metadata": {},
   "source": [
    "*This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\n",
    "and decreasing it thereafter proportionally to the inverse square root of the step number*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScheduler(): \n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps=4000):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.cur_step = 0\n",
    "        self.cur_lr = None\n",
    "\n",
    "        self.step()\n",
    "\n",
    "    def step(self): \n",
    "\n",
    "        self.cur_step += 1\n",
    "        self.cur_lr = self._get_lr()\n",
    "\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = self.cur_lr\n",
    "\n",
    "    def _get_lr(self):\n",
    "        return self.d_model ** (-0.5) * min(\n",
    "            self.cur_step ** (-0.5),\n",
    "            self.cur_step * self.n_warmup_steps ** (-1.5)\n",
    "        )\n",
    "\n",
    "    def get_last_lr(self):\n",
    "        return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63653b",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa4162",
   "metadata": {},
   "source": [
    "**Training Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MachineTranslationTransformer( \n",
    "    d_model = config['D_MODEL'],\n",
    "    n_blocks = config['N_BLOCKS'],\n",
    "    src_vocab_size = config['VOCAB_SIZE'],\n",
    "    trg_vocab_size = config['VOCAB_SIZE'],\n",
    "    n_heads = config['N_HEADS'],\n",
    "    d_ff = config['D_FF'],\n",
    "    dropout_proba = config['DROPOUT_PROBA']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e405a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss( \n",
    "    ignore_index = 0, label_smoothing = config['LABEL_SMOOTHING'], reduction = 'mean'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam( \n",
    "    model.parameters(), betas = config['BETAS'], eps = config['EPS']\n",
    ")\n",
    "\n",
    "scheduler = CustomScheduler( \n",
    "    optimizer, config['D_MODEL'], config['N_WARMUP_STEPS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ceacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['DEVICE'] == 'gpu': \n",
    "    device='cuda'\n",
    "else:\n",
    "    device='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f40ec",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ee7e51",
   "metadata": {},
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bfaf61",
   "metadata": {},
   "source": [
    "**Note:** This is a much simpler training loop then the one I implemented in src/learner.py. This training loop logs only training and validation loss. For training I highly suggest using the full source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.manual_seed(0)\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "cur_step = 1\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch_idx in range(config['EPOCHS']): \n",
    "    \n",
    "    train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dl): \n",
    "\n",
    "        xb, yb = batch\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        preds = model(xb,yb)\n",
    "        \n",
    "        loss = loss_func( \n",
    "            preds.reshape( -1, preds.size(-1) ), yb[:,1:].contiguous().view(-1)\n",
    "        )\n",
    "\n",
    "        train_loss += loss.detach().cpu()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        if cur_step % config['GRAD_ACCUMULATION_STEPS'] == 0: \n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            cur_step += 1\n",
    "\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for batch_idx, batch in enumerate(val_dl): \n",
    "\n",
    "            xb, yb = batch\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            preds = model(xb,yb)\n",
    "\n",
    "            loss = loss_func( \n",
    "                preds.reshape( -1, preds.size(-1) ), yb[:,1:].contiguous().view(-1)\n",
    "            )\n",
    "\n",
    "            val_loss += loss.detach().cpu()\n",
    "\n",
    "    print(f\"Train Loss: {train_loss}, Validation Loss: {val_loss}\")           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36092c4",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
