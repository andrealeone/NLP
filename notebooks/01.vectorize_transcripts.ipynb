{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f6f697",
   "metadata": {},
   "source": [
    "<br>\n",
    "<u>Notebook One</u> | \n",
    "<a href=https://leone.gdn/NLP target=_blank>Report</a> | \n",
    "<a href=https://github.com/andrealeone/NLP target=_blank>Repository</a>\n",
    "<br><br>\n",
    "<b>Transcript vectorisation</b><br><br>\n",
    "Andrea Leone<br>\n",
    "ML for NLP â€” University of Trento<br>\n",
    "January 2022\n",
    "<hr><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import project\n",
    "\n",
    "import numpy  as np\n",
    "import spacy\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3d359",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d46e1",
   "metadata": {},
   "source": [
    "Load the pre-trained pipelines for word embeddings:\n",
    "* [`en_core_web_lg`](https://spacy.io/models/en#en_core_web_lg): English tok2vec pipeline optimized for CPU.  \n",
    "Includes 685k unique vectors (300 dimensions); trained on [GloVe Common Crawl](https://nlp.stanford.edu/projects/glove/).\n",
    "* [`en_core_web_trf`](https://spacy.io/models/en#en_core_web_trf): English transformer pipeline based on [RoBERTa](https://github.com/pytorch/fairseq/tree/master/examples/roberta).<br>\n",
    "Includes no vectors, but can be obtained by extracting the transformer's internal embeddings (768 dimensions).\n",
    "\n",
    "Both pipelines are trained on [WordNet 3.0](https://wordnet.princeton.edu/) lexical database of English, [ClearNLP](https://github.com/clir/clearnlp-guidelines/blob/master/md/components/dependency_conversion.md) Constituent-to-Dependency Conversion, and [OntoNotes 5](https://catalog.ldc.upenn.edu/LDC2013T19) corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bc8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp  = spacy.load('en_core_web_lg')\n",
    "trf  = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbcc456",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e82f4e",
   "metadata": {},
   "source": [
    "### Static model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5890dcb",
   "metadata": {},
   "source": [
    "Query the records that still have no vector transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681190de",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = project.sql(\"\"\" \n",
    "    SELECT * FROM talks\n",
    "    WHERE\n",
    "        transcript IS NOT NULL AND\n",
    "        vector     IS NULL\n",
    "    ORDER BY slug ASC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d25470",
   "metadata": {},
   "source": [
    "For each record retrieved, get the transcript, input it in the `nlp` pipeline to vectorise the entire document (token-per-token), extract the document vector converting the numerical values to `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba237d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in tqdm( records ): \n",
    "\n",
    "    slug       = record[0]\n",
    "    transcript = record[4]\n",
    "\n",
    "    vector     = nlp( transcript ).vector.astype( np.float64 )\n",
    "    vector     = project.sqlize_array( vector )\n",
    "\n",
    "    project.sql_commit(\"UPDATE talks SET vector='{0}' WHERE slug='{1}'\".format(vector, slug))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae497ce3",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21f84d",
   "metadata": {},
   "source": [
    "### Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49faefb",
   "metadata": {},
   "source": [
    "Query the records that still have no vectorised transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d401d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = project.sql(\"\"\" \n",
    "    SELECT * FROM talks\n",
    "    WHERE\n",
    "        transcript IS NOT NULL AND\n",
    "        vector_trf IS NULL\n",
    "    ORDER BY slug ASC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9322cf72",
   "metadata": {},
   "source": [
    "As transformer-based pretrained models work at tensor-level, they eventually need to be re-aligned to the tokens to extract word/span/document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language \n",
    "\n",
    "@Language.factory('tensor2attr')\n",
    "class Tensor2Attr:\n",
    "\n",
    "    def __init__(self, name, nlp):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        self.add_attributes(doc)\n",
    "        return doc\n",
    "\n",
    "    def add_attributes(self, doc):\n",
    "        doc.user_hooks['vector']           = self.doc_tensor\n",
    "        doc.user_span_hooks['vector']      = self.span_tensor\n",
    "        doc.user_token_hooks['vector']     = self.token_tensor\n",
    "        doc.user_hooks['similarity']       = self.get_similarity\n",
    "        doc.user_span_hooks['similarity']  = self.get_similarity\n",
    "        doc.user_token_hooks['similarity'] = self.get_similarity\n",
    "\n",
    "    def doc_tensor(self, doc):\n",
    "        return doc._.trf_data.tensors[-1].mean(axis=0)\n",
    "\n",
    "    def span_tensor(self, span):\n",
    "        tensor_ix = span.doc._.trf_data.align[span.start: span.end].data.flatten()\n",
    "        out_dim   = span.doc._.trf_data.tensors[0].shape[-1]\n",
    "        tensor    = span.doc._.trf_data.tensors[0].reshape(-1, out_dim)[tensor_ix]\n",
    "        return tensor.mean(axis=0)\n",
    "\n",
    "    def token_tensor(self, token):\n",
    "        tensor_ix = token.doc._.trf_data.align[token.i].data.flatten()\n",
    "        out_dim   = token.doc._.trf_data.tensors[0].shape[-1]\n",
    "        tensor    = token.doc._.trf_data.tensors[0].reshape(-1, out_dim)[tensor_ix]\n",
    "        return tensor.mean(axis=0)\n",
    "\n",
    "    def get_similarity(self, doc1, doc2):\n",
    "        return np.dot(doc1.vector, doc2.vector) / (doc1.vector_norm * doc2.vector_norm)\n",
    "\n",
    "trf.add_pipe('tensor2attr')\n",
    "trf.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25445fc2",
   "metadata": {},
   "source": [
    "For each record retrieved, get the transcript, input it in the `trf` pipeline to vectorise the entire document using the transformer, align the tensors with the tokens with the custom task, and extract the document vector converting the numerical values to `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in tqdm( records ): \n",
    "\n",
    "    slug       = record[0]\n",
    "    transcript = record[4]\n",
    "\n",
    "    vector     = trf( transcript ).vector.astype( np.float64 )\n",
    "    vector     = project.sqlize_array( vector )\n",
    "\n",
    "    project.sql_commit(\"UPDATE talks SET vector_trf='{0}' WHERE slug='{1}'\".format(vector, slug))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b45a275",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
